# Introducción

	Mediante el presente proyecto se pretende implementar una solución al famoso juego Snake, en el cual nos encontramos en un entorno cuadrado donde aparecen "manzanas" que debemos recolectar para poder hacer crecer de tamaño nuestro personaje, que resulta, como el nombre del juego lo indica, una serpiente cuyo cuerpo se agranda a medida que se van recolectando dichas manzanas. El personaje no debe colisionar con paredes o con su propio cuerpo, ya que esto significa que se vuelva a la puntuación inicial perdiendo todo el progreso

 	La premisa del videojuego resulta sencilla, desarrollar un entramado de "if else" puede llegar a resolver el problema y, aunque no demasiado elegante, llegaríamos a una solución razonable. El gran problema de este acercamiento es que nuestro programa nunca podría mejorar sus habilidades mas allá de aquellas con las que fue diseñado en primer lugar, por mas que nuestro programa tropiece una y otra vez con su cuerpo o una pared, este no aprenderá de dichos sucesos, incluso, tendremos el grave problema de que el personaje se comportará exactamente igual ante situaciones similares y por tanto, siempre tendrá los mismos aciertos y errores. Por otro lado, la eficacia del programa depende enteramente de la habilidad del programador en el juego en cuestión, ya que deben definirse reglas de comportamiento para cada una de las situaciones que se puedan llegar a dar en el juego, y si bien en este caso puede no parecer tan extensa, no resulta fácil definir una regla para cada una de las situaciones que se puedan llegar a dar en el juego.
	
	A causa de todo lo nombrado anteriormente, decidimos que el problema se debía resolver mediante técnicas de aprendizaje reforzado, ya que estas nos brindan la posibilidad de desplegar un agente dentro del entorno el cual tenga capacidad de generar reglas de comportamiento propias en base a sus experiencias mientras juega, basado principalmente en un sistema de recompensas que el propio entorno provee cada vez que el agente realiza una acción. De esta forma, nuestro programa (ahora denominado agente) puede superar ampliamente las habilidades de quien lo programa dentro del propio juego, ya que con suficiente práctica, podría llegar a generar reglas de comportamiento generales y aplicarlas a cada momento, reglas que incluso aquel que lo programó desconocía.

	En las siguientes secciones se explicarán todos aquellos conceptos teóricos utilizados para la formulación de nuestra solución y se explicarán nuestras implementaciones para resolver el problema, así como comparaciones entre los distintos algoritmos subyacentes a cada una de las implementaciones.

# Marco teórico


	Los siguientes conceptos resultan fundamentales para explicación e implementación de todas las técnicas que se utilizaran para resolver el problema, las cuales serán explicadas en orden, comenzando por los conceptos mas teóricos como lo son los los SDP (Sequential Decision Problems por sus siglas en inglés), MDP (Markov Decision Process por sus siglas en inglés), siguiendo por aquellos conceptos cercanos a la implementacion como son el caso del algoritmo de Q-Learning y la utilización de redes neuronales en lo que se denomina DRL (Deep Reinforcement Learning por sus siglas en inglés).

	Uno de los patrones que presentan todos aquellos problemas en los que se requiere de aprendizaje en base a la experiencia es la presencia de un conjunto de acciones que se pueden llegar a tomar, en conjunto con determinados estados en los que se puede llegar a estar según la situación. El desafío resulta de tener que tomar decisiones correctas con respecto a que acción realizar en función del estado en donde nos encontremos y a donde queramos llegar. Esto nos trae inmediatamente a lo que se denomina un SDP, el cual resulta un problema en el cual nuestro personaje (a partir de ahora llamado agente) debe llegar a un estado deseado mediante la realización de un conjunto de acciones ordenadas y elegidas en secuencia sobre un entorno, pero bien ¿Como podemos lograr que nuestro agente tome las decisiones correctas en un entorno? 

	En primer lugar tenemos que definir un tipo de entorno para nuestro agente, para el total de los casos tratados en el presente informe se tendrán en cuenta entornos completamente determinísticos, los cuales resultan estáticos y no dependen de ningún tipo de azar, es decir, si al agente le está permitido desplazarse hacia arriba en algún tipo de tablero, este podrá el 100% de los casos sin ningún posible impedimento. Nuestro entorno también será definido siempre como completamente observable, es decir que el agente sabe a todo momento en que estado se encuentra, además de saber las posibles acciones que puede llegar a tomar.

	Una vez definido el entorno, podemos retomar nuestra pregunta sobre aprendizaje de nuestro agente. El primer acercamiento que posiblemente se nos ocurra como seres humanos es el de la prueba y error, es decir, si queremos aprender sobre cualquier cosa, debemos intentar tomar acciones y ver los resultados para así poder tomar decisiones sobre que acción tomar según el estado en el que nos encontremos, para así, encontrar la secuencia de acciones que nos lleve a conseguir nuestro objetivo, ya sea llegar a alguna posición en particular o golpear correctamente la pelota en algún deporte.

	El principio que rige el aprendizaje reforzado es el denominado MDP (Markov Decision Process), el cual se define teóricamente mediante un conjunto de acciones denominado: ‘a’, el cual representa el conjunto de acciones realizables por nuestro agente dentro del entorno, un conjunto de estados denominado: ‘s’, el cual contiene todos los posibles estados en los que el agente puede llegar a encontrarse según su situación en el entorno, también, se define el modelo de transición que se representa mediante una función P(s,a) = s’, siendo ‘s’ el estado actual del agente y ‘a’ la acción que el agente toma a continuación, devolviendo “ s’ ” que resulta el nuevo estado en el que el agente se encuentra, por último, se definen las recompensas, estas resultan una parte fundamental dentro del aprendizaje del agente, ya que informan al agente que tan “deseable” resulta una acción tomada y que tan “deseable” resulta estar en un estado específico ( capítulo 17 sección 1 de AIMA ).

	En base a nuestro MDP, definido generalmente por las propiedades del problema, para llegar  a una solución debemos encontrar lo que se denomina una “Política óptima”, una política representa la forma en la cual el agente se comporta en determinadas situaciones, básicamente actúa como una especie de conciencia que le dice que hacer basado en las probabilidades de obtener una mayor recompensa con sus acciones. Se dice que esta política es óptima, si para cada paso que da el agente  se está tomando una decisión que favorece a la recompensa total obtenida por el agente luego de conseguir el objetivo.

	En torno a encontrar la denominada “política óptima” es en lo que trabajan cada uno de los algoritmos de aprendizaje reforzado, para lograr encontrarla, los algoritmos trabajan con un conjunto de conceptos para trabajar su “memoria”, en este caso memoria se refiere a la capacidad del agente de almacenar el conocimiento adquirido durante su experiencia en el entorno para posteriormente utilizarlo a la hora de tomar acciones que maximicen la recompensa. Las recompensas se otorgan al agente en función del problema a resolver, esto se profundizará en las secciones correspondientes a nuestra implementación para resolver el juego de “Snake”, donde se explicará en profundidad como se distribuyen las recompensas para facilitar el aprendizaje del agente en este entorno específico.

	Como ya se explicó, la idea del aprendizaje reforzado es la experimentación del agente en el entorno para así entender las reglas del mismo y resolver un problema determinado, el proceso que los algoritmos utilizan para hacer que el agente “experimente” se denomina episodio. Los episodios se refieren, en resumidas cuentas, a una secuencia de acciones que el agente tomó hasta llegar a un estado terminal, es decir, tuvo “éxito” o “fracasó”. Para que pueda entenderse con claridad, si se estuviera entrenando a un agente para encontrar la ruta mas corta hacia un punto dado, uno de los estados terminales estaría dado por haber llegado a la meta y otro por haberse quedado sin tiempo, estos estados finales marcan la terminación de una secuencia de acciones que llevó al agente a un éxito o fracaso.

	Para realizar el aprendizaje, el algoritmo evalúa el rendimiento del agente en cada uno de estos episodios. El rendimiento se evalúa mediante la sumatoria de las recompensas obtenidas por el agente a lo largo de la secuencia de acciones que este realizó durante un episodio, así, si el agente tuvo un buen rendimiento en determinado episodio, la suma total de recompensas será mas alta que en aquel en el que no llegó al objetivo o tomó un camino mas largo (en caso de un agente siendo entrenado para encontrar el camino mas corto de un punto a otro)

	Cuando nuestro agente se encuentra en una posición determinada, puede hacer uso de su conocimiento del entorno para calcular mediante probabilidades, cual será el camino que mayor recompensa le brindará, esta estimación en conjunto con el conocimiento adquirido durante los episodios se puede realizar de dos maneras: (también llamadas funciones de utilidad, página 649 AIMA)

1)  Recompensa aditiva: R([s0,s1,s2,…]) = R(s0)  + R(S1) + …

	La función R representa la recompensa total estimada a la hora de tomar un conjunto de acciones en el futuro (o en el pasado en caso de estar calculando la recompensa de un episodio) y [s0,s1,s2,…] representan los estados en los que el agente va a encontrarse luego de tomar determinadas acciones.

	Esta función resulta ser “inocente”, ya que no toma en cuenta la cercanía de las acciones al momento en el tiempo donde se encuentra el agente, es decir, la recompensa aporta lo mismo ya sea que se tome dentro de 10 unidades de tiempo o en la siguiente, lo que puede ser perjudicial y tomar una decisión inmediata demasiado negativa en la cercanía de tiempo, siendo que el agente se está arriesgando a una estimación realizada sobre una recompensa a largo plazo que puede o no ser correcta. Una analogía podría ser el mundo de las finanzas, si se estima que el bitcoin a 10 años va a costar 100 veces mas, pero hoy en día resulta una pésima inversión, corremos el riesgo de que nuestra predicción sea errónea y perdamos mucho dinero.

2) Recompensa descontada: R([s0,s1,s2,…]) = R(s0)  + a*R(s1) + a^2*R(s2)

	Para el cálculo de esta función se introduce un valor numérico entre 0 y 1 llamado factor de descuento, el cual representa la preferencia que puede tendrá el agente por la recompensa inmediata por sobre la recompensa a largo plazo, evitando así, el problema definido anteriormente.

