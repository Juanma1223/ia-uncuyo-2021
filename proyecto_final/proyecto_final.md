# Introducción

	Mediante el presente proyecto se pretende implementar una solución al famoso juego Snake, en el cual nos encontramos en un entorno cuadrado donde aparecen "manzanas" que debemos recolectar para poder hacer crecer de tamaño nuestro personaje, que resulta, como el nombre del juego lo indica, una serpiente cuyo cuerpo se agranda a medida que se van recolectando dichas manzanas. El personaje no debe colisionar con paredes o con su propio cuerpo, ya que esto significa que se vuelva a la puntuación inicial perdiendo todo el progreso

 	La premisa del videojuego resulta sencilla, desarrollar un entramado de "if else" puede llegar a resolver el problema y, aunque no demasiado elegante, llegaríamos a una solución razonable. El gran problema de este acercamiento es que nuestro programa nunca podría mejorar sus habilidades mas allá de aquellas con las que fue diseñado en primer lugar, por mas que nuestro programa tropiece una y otra vez con su cuerpo o una pared, este no aprenderá de dichos sucesos, incluso, tendremos el grave problema de que el personaje se comportará exactamente igual ante situaciones similares y por tanto, siempre tendrá los mismos aciertos y errores. Por otro lado, la eficacia del programa depende enteramente de la habilidad del programador en el juego en cuestión, ya que deben definirse reglas de comportamiento para cada una de las situaciones que se puedan llegar a dar en el juego, y si bien en este caso puede no parecer tan extensa, no resulta fácil definir una regla para cada una de las situaciones que se puedan llegar a dar en el juego.
	
	A causa de todo lo nombrado anteriormente, decidimos que el problema se debía resolver mediante técnicas de aprendizaje reforzado, ya que estas nos brindan la posibilidad de desplegar un agente dentro del entorno el cual tenga capacidad de generar reglas de comportamiento propias en base a sus experiencias mientras juega, basado principalmente en un sistema de recompensas que el propio entorno provee cada vez que el agente realiza una acción. De esta forma, nuestro programa (ahora denominado agente) puede superar ampliamente las habilidades de quien lo programa dentro del propio juego, ya que con suficiente práctica, podría llegar a generar reglas de comportamiento generales y aplicarlas a cada momento, reglas que incluso aquel que lo programó desconocía.

	En las siguientes secciones se explicarán todos aquellos conceptos teóricos utilizados para la formulación de nuestra solución y se explicarán nuestras implementaciones para resolver el problema, así como comparaciones entre los distintos algoritmos subyacentes a cada una de las implementaciones.

# Marco teórico


## Conceptos base

	Los siguientes conceptos resultan fundamentales para explicación e implementación de todas las técnicas que se utilizaran para resolver el problema, las cuales serán explicadas en orden, comenzando por los conceptos mas teóricos como lo son los los SDP (Sequential Decision Problems por sus siglas en inglés), MDP (Markov Decision Process por sus siglas en inglés), siguiendo por aquellos conceptos cercanos a la implementacion como son el caso del algoritmo de Q-Learning y la utilización de redes neuronales en lo que se denomina DRL (Deep Reinforcement Learning por sus siglas en inglés).

	Uno de los patrones que presentan todos aquellos problemas en los que se requiere de aprendizaje en base a la experiencia es la presencia de un conjunto de acciones que se pueden llegar a tomar, en conjunto con determinados estados en los que se puede llegar a estar según la situación. El desafío resulta de tener que tomar decisiones correctas con respecto a que acción realizar en función del estado en donde nos encontremos y a donde queramos llegar. Esto nos trae inmediatamente a lo que se denomina un SDP, el cual resulta un problema en el cual nuestro personaje (a partir de ahora llamado agente) debe llegar a un estado deseado mediante la realización de un conjunto de acciones ordenadas y elegidas en secuencia sobre un entorno, pero bien ¿Como podemos lograr que nuestro agente tome las decisiones correctas en un entorno? 

	En primer lugar tenemos que definir un tipo de entorno para nuestro agente, para el total de los casos tratados en el presente informe se tendrán en cuenta entornos completamente determinísticos, los cuales resultan estáticos y no dependen de ningún tipo de azar, es decir, si al agente le está permitido desplazarse hacia arriba en algún tipo de tablero, este podrá el 100% de los casos sin ningún posible impedimento. Nuestro entorno también será definido siempre como completamente observable, es decir que el agente sabe a todo momento en que estado se encuentra, además de saber las posibles acciones que puede llegar a tomar.

	Una vez definido el entorno, podemos retomar nuestra pregunta sobre aprendizaje de nuestro agente. El primer acercamiento que posiblemente se nos ocurra como seres humanos es el de la prueba y error, es decir, si queremos aprender sobre cualquier cosa, debemos intentar tomar acciones y ver los resultados para así poder tomar decisiones sobre que acción tomar según el estado en el que nos encontremos, para así, encontrar la secuencia de acciones que nos lleve a conseguir nuestro objetivo, ya sea llegar a alguna posición en particular o golpear correctamente la pelota en algún deporte.

	El principio que rige el aprendizaje reforzado es el denominado MDP (Markov Decision Process), el cual se define teóricamente mediante un conjunto de acciones denominado: ‘a’, el cual representa el conjunto de acciones realizables por nuestro agente dentro del entorno, un conjunto de estados denominado: ‘s’, el cual contiene todos los posibles estados en los que el agente puede llegar a encontrarse según su situación en el entorno, también, se define el modelo de transición que se representa mediante una función P(s,a) = s’, siendo ‘s’ el estado actual del agente y ‘a’ la acción que el agente toma a continuación, devolviendo “ s’ ” que resulta el nuevo estado en el que el agente se encuentra, por último, se definen las recompensas, estas resultan una parte fundamental dentro del aprendizaje del agente, ya que informan al agente que tan “deseable” resulta una acción tomada y que tan “deseable” resulta estar en un estado específico ( capítulo 17 sección 1 de AIMA ).

	En base a nuestro MDP, definido generalmente por las propiedades del problema, para llegar  a una solución debemos encontrar lo que se denomina una “Política óptima”, una política representa la forma en la cual el agente se comporta en determinadas situaciones, básicamente actúa como una especie de conciencia que le dice que hacer basado en las probabilidades de obtener una mayor recompensa con sus acciones. Se dice que esta política es óptima, si para cada paso que da el agente  se está tomando una decisión que favorece a la recompensa total obtenida por el agente luego de conseguir el objetivo.

	En torno a encontrar la denominada “política óptima” es en lo que trabajan cada uno de los algoritmos de aprendizaje reforzado, para lograr encontrarla, los algoritmos trabajan con un conjunto de conceptos para trabajar su “memoria”, en este caso memoria se refiere a la capacidad del agente de almacenar el conocimiento adquirido durante su experiencia en el entorno para posteriormente utilizarlo a la hora de tomar acciones que maximicen la recompensa. Las recompensas se otorgan al agente en función del problema a resolver, esto se profundizará en las secciones correspondientes a nuestra implementación para resolver el juego de “Snake”, donde se explicará en profundidad como se distribuyen las recompensas para facilitar el aprendizaje del agente en este entorno específico.

	Como ya se explicó, la idea del aprendizaje reforzado es la experimentación del agente en el entorno para así entender las reglas del mismo y resolver un problema determinado, el proceso que los algoritmos utilizan para hacer que el agente “experimente” se denomina episodio. Los episodios se refieren, en resumidas cuentas, a una secuencia de acciones que el agente tomó hasta llegar a un estado terminal, es decir, tuvo “éxito” o “fracasó”. Para que pueda entenderse con claridad, si se estuviera entrenando a un agente para encontrar la ruta mas corta hacia un punto dado, uno de los estados terminales estaría dado por haber llegado a la meta y otro por haberse quedado sin tiempo, estos estados finales marcan la terminación de una secuencia de acciones que llevó al agente a un éxito o fracaso.

	Para realizar el aprendizaje, el algoritmo evalúa el rendimiento del agente en cada uno de estos episodios. El rendimiento se evalúa mediante la sumatoria de las recompensas obtenidas por el agente a lo largo de la secuencia de acciones que este realizó durante un episodio, así, si el agente tuvo un buen rendimiento en determinado episodio, la suma total de recompensas será mas alta que en aquel en el que no llegó al objetivo o tomó un camino mas largo (en caso de un agente siendo entrenado para encontrar el camino mas corto de un punto a otro)

	Cuando nuestro agente se encuentra en una posición determinada, puede hacer uso de su conocimiento del entorno para calcular mediante probabilidades, cual será el camino que mayor recompensa le brindará, esta estimación en conjunto con el conocimiento adquirido durante los episodios se puede realizar de dos maneras: (también llamadas funciones de utilidad, página 649 AIMA)

1)  Recompensa aditiva: R([s0,s1,s2,…]) = R(s0)  + R(S1) + …

	La función R representa la recompensa total estimada a la hora de tomar un conjunto de acciones en el futuro (o en el pasado en caso de estar calculando la recompensa de un episodio) y [s0,s1,s2,…] representan los estados en los que el agente va a encontrarse luego de tomar determinadas acciones.

	Esta función resulta ser “inocente”, ya que no toma en cuenta la cercanía de las acciones al momento en el tiempo donde se encuentra el agente, es decir, la recompensa aporta lo mismo ya sea que se tome dentro de 10 unidades de tiempo o en la siguiente, lo que puede ser perjudicial y tomar una decisión inmediata demasiado negativa en la cercanía de tiempo, siendo que el agente se está arriesgando a una estimación realizada sobre una recompensa a largo plazo que puede o no ser correcta. Una analogía podría ser el mundo de las finanzas, si se estima que el bitcoin a 10 años va a costar 100 veces mas, pero hoy en día resulta una pésima inversión, corremos el riesgo de que nuestra predicción sea errónea y perdamos mucho dinero.

2) Recompensa descontada: R([s0,s1,s2,…]) = R(s0)  + a*R(s1) + a^2*R(s2)

	Para el cálculo de esta función se introduce un valor numérico entre 0 y 1 llamado factor de descuento, el cual representa la preferencia que puede tendrá el agente por la recompensa inmediata por sobre la recompensa a largo plazo, evitando así, el problema definido anteriormente.


## Algoritmos

	Cada uno de los conceptos citados anteriormente son los que se utilizan como base para los algoritmos de aprendizaje reforzado en mayor o menor medida. Son varios los algoritmos que se pueden llegar a utilizar para resolver un problema de aprendizaje reforzado, entre ellos se encuentran los siguientes:

### Fuerza Bruta: 

	Este algoritmo resulta el mas básico de todos, busca a través de probar todas las posibles combinaciones para obtener una política óptima, es decir, encontrar una función p(s) = ‘a’, tal que p es la política, ‘s’ el estado actual del agente y ‘a’ es la acción mas óptima que se puede realizar dada la situación actual. Generalmente funciona para problemas extremadamente simples en los cuales la política viene dada por una simple distribución de probabilidad de éxito al tomar cada acción. El algoritmo itera utilizando la ya mencionada “función de utilidad”, que estima la secuencia de acciones mediante la cual el agente maximizará la recompensa obtenida.

### Q-Learning: 

	Basado principalmente en los principios de programación dinámica, Q-Learning es un algoritmo que utiliza un conjunto de funciones para la exploración del entorno, luego, lo “aprende” o “memoriza” utilizando una matriz denominada Q-Table.

	La exploración en Q-Learning apunta a maximizar la recompensa obtenida a cada acción que toma, es decir, el agente intenta maximizar la denominada función “Expected Discounted Return” o Recompensa Descontada (definida con anterioridad en las funciones de utilidad) la cual se define en la siguiente figura


(Insertar la imagen en el markdown)
	https://miro.medium.com/max/778/1*uCvDmywJgMcaW6aYMkqblw.png

Fuente: https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187


	Para maximizar dicha función, el agente debe encontrar una política capaz de tomar decisiones, para lograrlo, se utiliza la Ecuación de Optimalidad de Bellman, la cual luce como se ven en la siguiente figura


(Insertar la imagen en markdown)
	https://miro.medium.com/max/846/1*t9CmhWlgbAqm8Y-fCgiw9w.png

Fuente: https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187


	En la figura se observan una serie de valores, además de la función q(s,a). La función ‘q’ o también llamada Q-Value es la que define que tan positivo es tomar determinada acción para el agente en un estado ‘s’ dado, si observamos la asignación al lado derecho nos encontramos una serie de valores que dictaminan el valor de tomar una acción en determinada situación, siendo Rt+1 la recompensa obtenida por el agente por tomar la acción ‘a’ para el siguiente movimiento, la letra griega ‘gamma’ el factor de descuento y una llamada recursiva a la función q con valores s’ y a’, pidiendo el máximo valor que encontramos con anterioridad en nuestra etapa de ‘exploración’.

	La utilización de la función se entiende aún mejor cuando introducimos el funcionamiento de nuestra Q-Table, matriz la cual tiene unas dimensiones definidas por el tamaño del conjunto de acciones y todos los posibles estados en los que el agente se puede encontrar. Formalmente hablando, dado un conjunto de acciones a = {a1,a2,a3,…, an} y un conjunto de estados en los que puede estar el agente dentro del entorrno s = {s1,s2,s3,…,sm}, nuestra matriz Q-Table tendrá una dimensión de n x m.

	En nuestra Q-Table, es donde se almacena el conocimiento del agente y también donde se guarda el valor de recompensa de cada uno de los episodios, utilizando lo que se llama una función de decaimiento en conjunto con la ecuación de Bellman, para así iterar sobre los episodios del agente dentro del entorno y balancear el uso del conocimiento previo con la exploración del mismo sobre el entorno. La función de decaimiento no es mas que aquella que nos aporta la probabilidad de que el agente explore dado un número de iteraciones o episodios pasados, es decir, a medida que vamos aprendiendo de nuestro entorno nos interesa que nuestro agente explore un poco menos y utilice su conocimiento aprendido, por tanto nuestra función proporciona una probabilidad baja de explorar, en contraparte, si nos encontramos en etapas tempranas de la exploración del entorno, nuestro conocimiento previo sobre el entorno es escaso o nulo y por tanto, queremos que nuestro agente explore y experimente.

	Se dice que Q-Learning se encuentra fuertemente relacionado con la programación dinámica porque nuestra matriz Q-Table recuerda mucho a la utilización de cálculos pasados para ahorrar tiempo de cálculo en el futuro, pues en realidad esta resulta la base del algoritmo. Cada una de las casillas se calcula mediante la igualdad definida en la figura de la ecuación de Bellman en el proceso de exploración, es decir, si al tomar determinada acción conseguimos un valor de recompensa mas alto que el que ya estaba en dicha casilla (que almacenaba el resultado de tomar la misma decisión en algún episodio pasado) lo reemplazamos por la recompensa total acumulada durante el episodio sumada a la recompensa de realizar la acción seleccionada por el agente. En caso de encontrarnos en fases mas avanzadas de aprendizaje, el agente tendrá una alta probabilidad de utilizar el conocimiento ya adquirido y por tanto, se utilizará la ecuación de Bellman para obtener la acción que mejores resultados nos ha dado en determinado estado, es decir, de nuestra matriz Q-Table, elegimos la acción con mayor valor de recompensa para la fila del estado correspondiente.

	Iterando sobre los episodios y utilizando nuestra Q-Table, almacenamos entonces el conocimiento del agente sobre el entorno, resultando en un aprendizaje iterativo.

### Deep Reinforcement Learning: 

	
	Antes de comenzar con Deep Learning, cabe aclarar que resulta un tema de una extensión mucho mas amplia de lo que se mostrará en el presente informe, ya que en sí mismo, el “Deep Learning” resulta una rama propia de la Inteligencia Artificial y por tanto, conlleva una gran cantidad de conceptos que exceden el alcance de este trabajo. Por lo dicho anteriormente, se describirán aquellos elementos básicos del Deep Learning a fin de explicar nuestra implementación del mismo para el problema que nos concierne.

	El funcionamiento de los algoritmos de esta rama de la Inteligencia Artificial se basa principalmente en las denominadas redes neuronales, las cuales están compuestas por una serie de “capas”, encargadas de procesar la información de manera tal que cada capa realiza una serie de operaciones sobre los datos y luego los propaga a la siguiente capa, para así repetir el proceso y llegar a una salida.

	La idea del Deep Learning es la de lograr una representación mas “simple” de los datos, realizando operaciones sobre la entrada y ajustando valores internos dentro de cada una de las capas, estos valores internos (que son en definitiva quienes tienen la “memoria” del modelo) se denominan pesos o variables ocultas, los cuales resultan del ajuste iterativo haciendo uso de una función de optimización y otra de “pérdida”. La función de pérdida (por su traducción del inglés Loss Function) es la encargada comparar la respuesta de la red en contraposición con la esperada, evaluando el buen o mal funcionamiento de la misma, es en base al resultado a esta función que actúa la función de optimización, ya que la función de perdida puntúa que tan cerca estuvo la predicción de la red al resultado esperado, por otro lado, la función de optimización, es la encargada de propagar cambios pertinentes en los pesos dentro de cada una de las capas. La función de optimización se basa en la obtención del gradiente de las operaciones que se realizan dentro de las capas de la red neuronal, siendo el gradiente un vector con la dirección de mayor crecimiento de una función en punto determinado, sabiendo esto, nosotros pretendemos que nuestra función de perdida sea lo mas baja posible para reducir la distancia entre el valor esperado y el valor que resultó de la red, por tanto, aplicamos nuestra función de optimización para modificar los pesos de las capas en dirección contraria al gradiente para así reducir el valor de la pérdida.

	Pero bien, la pregunta ahora resulta ¿ Como se tratan los datos dentro de la red ? Los datos se encuentran presentes como tensores, los cuales son una estructura matemática capaz de representar múltiples datos de múltiples dimensiones. Hay variedad de operaciones definidas para los tensores, entre ellas la suma y la multiplicación, en conjunto con otras operaciones llamadas funciones de activación. La forma mas común de funcionamiento es mediante aplicar el operador suma y/o multiplicación sobre los datos de entrada de la capa, en conjunto con los valores que esta almacena, es decir, los pesos, para finalmente aplicar algún tipo de función de activación, que introduce cambios deseados con el fin de obtener representaciones útiles para las futuras capas.

	Luego de describir a grandes rasgos el funcionamiento de las redes neuronales, se procede a explicar como estas se pueden aplicar al terreno del aprendizaje reforzado. Como ya sabemos, en el aprendizaje reforzado no poseemos tal cosa como datos de prueba, datos de validación y datos de entrenamiento, por tanto, la entrada a nuestra red neuronal jamás será un conjunto de datos cuya predicción de la red se compare con una salida esperada como es en el caso del aprendizaje supervisado por ejemplo. En este caso, nuestra entrada a la red neuronal resulta ser los estados en los que el agente se encuentra a cada momento, es decir, a cada acción que este realiza con el paso del tiempo, nuestra salida de la red neuronal ahora será la próxima acción que el agente deberá realizar y finalmente, nuestra función de pérdida vendrá definida por la recompensa que el entorno nos devuelva al realizar determinada acción, comparándola con la máxima esperada.

	La manera del agente de almacenar su conocimiento, no resulta otra que los pesos que se encuentran en cada una de las capas, es decir, su representación del entorno y la función de transición de estado del agente vienen definidas por como se han ajustado los pesos en la red neuronal según un conjunto de experiencias previas que el mismo ha experimentado. A diferencia de Q-Learning, en el cual teníamos acceso al conocimiento del agente mediante el acceso a las celdas de la Q-Table, en el caso de nuestra red neuronal se actúa como una especie de caja negra, ya que aunque accedamos a los pesos de nuestras capas, la realidad es que estos serán una representación abstracta del conocimiento que el agente tiene del entorno, ya que nosotros no podemos entender como el agente a aprendido los conceptos y reglamentos que se le presentan y como debe ejecutar las acciones correctas según las recompensas obtenidas. Además, las redes neuronales son conocidas por generar lo que se llama sobre aprendizaje u overfitting en inglés, debido a su alta flexibilidad a la hora de adquirir conocimiento, algo de lo que los métodos mas simples se salvan por sus representaciones mediante funciones mas sencillas. Es por esto último, que se debe tener especial cuidado con la forma de entrenar una red neuronal para evitar que luego el conocimiento adquirido no se pueda generalizar. Si esto mismo lo vemos desde el punto de vista del aprendizaje reforzado, el sobre aprendizaje podría generar que el agente solo sepa tomar acciones correctas en los límites en los que se lo entrenó, pero para aquellas situaciones que no ha experimentado directamente, este puede comportarse de manera errática.
	